<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Targeted Learning with Stochastic Treatment Regimes • tmle3shift</title>
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://cdnjs.cloudflare.com/ajax/libs/bootswatch/3.4.0/flatly/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="Targeted Learning with Stochastic Treatment Regimes">
<meta property="og:description" content="tmle3shift">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]--><!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=UA-115145808-1"></script><script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-115145808-1');
</script>
</head>
<body data-spy="scroll" data-target="#toc">
    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">tmle3shift</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="Released version">0.2.0</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="https://tlverse.org">tlverse</a>
</li>
<li>
  <a href="../index.html">tmle3</a>
</li>
<li>
  <a href="../articles/shift_tmle.html">Overview</a>
</li>
<li>
  <a href="../articles/index.html">Articles</a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/tlverse/tmle3shift">
    <span class="fas fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>Targeted Learning with Stochastic Treatment Regimes</h1>
                        <h4 class="author">
<a href="https://nimahejazi.org">Nima Hejazi</a>, <a href="https://github.com/jeremyrcoyle">Jeremy Coyle</a>, and <a href="https://vanderlaan-lab.org">Mark van der Laan</a>
</h4>
            
            <h4 class="date">2021-03-13</h4>
      
      <small class="dont-index">Source: <a href="https://github.com/tlverse/tmle3shift/blob/master/vignettes/shift_tmle.Rmd"><code>vignettes/shift_tmle.Rmd</code></a></small>
      <div class="hidden name"><code>shift_tmle.Rmd</code></div>

    </div>

    
    
<div id="introduction" class="section level2">
<h2 class="hasAnchor">
<a href="#introduction" class="anchor"></a>Introduction</h2>
<p>Stochastic treatment regimes present a relatively simple manner in which to assess the effects of continuous treatments by way of parameters that examine the effects induced by the counterfactual shifting of the observed values of a treatment of interest. Here, we present an implementation of a new algorithm for computing targeted minimum loss-based estimates of treatment shift parameters defined based on a shifting function <span class="math inline">\(d(A,W)\)</span>. For a technical presentation of the algorithm, the interested reader is invited to consult <span class="citation">Dı́az and van der Laan (2018)</span>. For additional background on Targeted Learning and previous work on stochastic treatment regimes, please consider consulting <span class="citation">van der Laan and Rose (2011)</span>, <span class="citation">van der Laan and Rose (2018)</span>, and <span class="citation">Dı́az and van der Laan (2012)</span>.</p>
<p>To start, let’s load the packages we’ll use and set a seed for simulation:</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://r-datatable.com">data.table</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://tlverse.org/sl3">sl3</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://tlverse.org/tmle3">tmle3</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://tlverse.org/tmle3shift">tmle3shift</a></span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">429153</span><span class="op">)</span></code></pre></div>
</div>
<div id="data-and-notation" class="section level2">
<h2 class="hasAnchor">
<a href="#data-and-notation" class="anchor"></a>Data and Notation</h2>
<p>Consider <span class="math inline">\(n\)</span> observed units <span class="math inline">\(O_1, \ldots, O_n\)</span>, where each random variable <span class="math inline">\(O = (W, A, Y)\)</span> corresponds to a single observational unit. Let <span class="math inline">\(W\)</span> denote baseline covariates (e.g., age, sex, education level), <span class="math inline">\(A\)</span> an intervention variable of interest (e.g., nutritional supplements), and <span class="math inline">\(Y\)</span> an outcome of interest (e.g., disease status). Though it need not be the case, let <span class="math inline">\(A\)</span> be continuous-valued, i.e. <span class="math inline">\(A \in \mathbb{R}\)</span>. Let <span class="math inline">\(O_i \sim \mathcal{P} \in \mathcal{M}\)</span>, where <span class="math inline">\(\mathcal{M}\)</span> is the nonparametric statistical model defined as the set of continuous densities on <span class="math inline">\(O\)</span> with respect to some dominating measure. To formalize the definition of stochastic interventions and their corresponding causal effects, we introduce a nonparametric structural equation model (NPSEM), based on <span class="citation">Pearl (2000)</span>, to define how the system changes under posited interventions: <span class="math display">\[\begin{align*}\label{eqn:npsem}
  W &amp;= f_W(U_W) \\ A &amp;= f_A(W, U_A) \\ Y &amp;= f_Y(A, W, U_Y),
\end{align*}\]</span> We denote the observed data structure <span class="math inline">\(O = (W, A, Y)\)</span></p>
<p>Letting <span class="math inline">\(A\)</span> denote a continuous-valued treatment, we assume that the distribution of <span class="math inline">\(A\)</span> conditional on <span class="math inline">\(W = w\)</span> has support in the interval <span class="math inline">\((l(w), u(w))\)</span> – for convenience, let this support be <em>a.e.</em> That is, the minimum natural value of treatment <span class="math inline">\(A\)</span> for an individual with covariates <span class="math inline">\(W = w\)</span> is <span class="math inline">\(l(w)\)</span>; similarly, the maximum is <span class="math inline">\(u(w)\)</span>. Then, a simple stochastic intervention, based on a shift <span class="math inline">\(\delta\)</span>, may be defined <span class="math display">\[\begin{equation}\label{eqn:shift}
  d(a, w) =
  \begin{cases}
    a - \delta &amp; \text{if } a &gt; l(w) + \delta \\
    a &amp; \text{if } a \leq l(w) + \delta,
  \end{cases}
\end{equation}\]</span> where <span class="math inline">\(0 \leq \delta \leq u(w)\)</span> is an arbitrary pre-specified value that defines the degree to which the observed value <span class="math inline">\(A\)</span> is to be shifted, where possible. For the purpose of using such a shift in practice, the present software provides the functions <code>shift_additive</code> and <code>shift_additive_inv</code>, which define a variation of this shift, assuming that the density of treatment <span class="math inline">\(A\)</span>, conditional on the covariates <span class="math inline">\(W\)</span>, has support <em>a.e.</em></p>
<div id="simulate-data" class="section level3">
<h3 class="hasAnchor">
<a href="#simulate-data" class="anchor"></a>Simulate Data</h3>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># simulate simple data for tmle-shift sketch</span>
<span class="va">n_obs</span> <span class="op">&lt;-</span> <span class="fl">1000</span> <span class="co"># number of observations</span>
<span class="va">n_w</span> <span class="op">&lt;-</span> <span class="fl">1</span> <span class="co"># number of baseline covariates</span>
<span class="va">tx_mult</span> <span class="op">&lt;-</span> <span class="fl">2</span> <span class="co"># multiplier for the effect of W = 1 on the treatment</span>

<span class="co">## baseline covariates -- simple, binary</span>
<span class="va">W</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/numeric.html">as.numeric</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/lapply.html">replicate</a></span><span class="op">(</span><span class="va">n_w</span>, <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">rbinom</a></span><span class="op">(</span><span class="va">n_obs</span>, <span class="fl">1</span>, <span class="fl">0.5</span><span class="op">)</span><span class="op">)</span><span class="op">)</span>

<span class="co">## create treatment based on baseline W</span>
<span class="va">A</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/numeric.html">as.numeric</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n_obs</span>, mean <span class="op">=</span> <span class="va">tx_mult</span> <span class="op">*</span> <span class="va">W</span>, sd <span class="op">=</span> <span class="fl">1</span><span class="op">)</span><span class="op">)</span>

<span class="co">## create outcome as a linear function of A, W + white noise</span>
<span class="va">Y</span> <span class="op">&lt;-</span> <span class="va">A</span> <span class="op">+</span> <span class="va">W</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n_obs</span>, mean <span class="op">=</span> <span class="fl">0</span>, sd <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span></code></pre></div>
<p>The above composes our observed data structure <span class="math inline">\(O = (W, A, Y)\)</span>. To formally express this fact using the <code>tlverse</code> grammar introduced by the <a href="https://github.com/tlverse/tmle3"><code>tmle3</code> package</a>, we create a single data object and specify the functional relationships between the nodes in the <em>directed acyclic graph</em> (DAG) via <em>nonparametric structural equation models</em> (NPSEMs), reflected in the node list that we set up:</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># organize data and nodes for tmle3</span>
<span class="va">data</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://Rdatatable.gitlab.io/data.table/reference/data.table.html">data.table</a></span><span class="op">(</span><span class="va">W</span>, <span class="va">A</span>, <span class="va">Y</span><span class="op">)</span>
<span class="va">node_list</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>W <span class="op">=</span> <span class="st">"W"</span>, A <span class="op">=</span> <span class="st">"A"</span>, Y <span class="op">=</span> <span class="st">"Y"</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">data</span><span class="op">)</span></code></pre></div>
<pre><code>##    W          A          Y
## 1: 1  2.4031607  3.7157578
## 2: 1  4.4973744  5.9651611
## 3: 1  2.0330871  2.2531970
## 4: 0 -0.8089023 -0.8849531
## 5: 1  1.8432067  2.7193091
## 6: 1  1.3555863  2.5705832</code></pre>
<p>We now have an observed data structure (<code>data</code>) and a specification of the role that each variable in the data set plays as the nodes in a DAG.</p>
</div>
</div>
<div id="methodology" class="section level2">
<h2 class="hasAnchor">
<a href="#methodology" class="anchor"></a>Methodology</h2>
<p>To start, we will initialize a specification for the TMLE of our parameter of interest (called a <code>tmle3_Spec</code> in the <code>tlverse</code> nomenclature) simply by calling <code>tmle_shift</code>. We specify the argument <code>shift_val = 0.5</code> when initializing the <code>tmle3_Spec</code> object to communicate that we’re interested in a shift of <span class="math inline">\(0.5\)</span> on the scale of the treatment <span class="math inline">\(A\)</span> – that is, we specify <span class="math inline">\(\delta = 0.5\)</span> (note that this is an arbitrarily chosen value for this example).</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># initialize a tmle specification</span>
<span class="va">tmle_spec</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/tmle_shift.html">tmle_shift</a></span><span class="op">(</span>shift_val <span class="op">=</span> <span class="fl">0.5</span>,
                        shift_fxn <span class="op">=</span> <span class="va">shift_additive</span>,
                        shift_fxn_inv <span class="op">=</span> <span class="va">shift_additive_inv</span><span class="op">)</span></code></pre></div>
<p>As seen above, the <code>tmle_shift</code> specification object (like all <code>tmle3_Spec</code> objects) does <em>not</em> store the data for our specific analysis of interest. Later, we’ll see that passing a data object directly to the <code>tmle3</code> wrapper function, alongside the instantiated <code>tmle_spec</code>, will serve to construct a <code>tmle3_Task</code> object internally (see the <a href="https://tlverse.org/tmle3"><code>tmle3</code> documentation</a> for details). Note that, by default, the <code>tmle_spec</code> object is set up to facilitate cross-validated estimation of likelihood components, ensuring certain empirical process conditions may be circumvented by reducing the contribution of an empirical process term to the estimated influence function <span class="citation">(Zheng and Laan 2011)</span>. In practice, this automatic incorporation of cross-validation (CV-TMLE) means that the user need not be concerned with these theoretical conditions being satisfied; moreover, cross-validated estimation of the efficient influence function is expected to control the estimated variance.</p>
<div id="interlude-constructing-optimal-stacked-regressions-with-sl3" class="section level3">
<h3 class="hasAnchor">
<a href="#interlude-constructing-optimal-stacked-regressions-with-sl3" class="anchor"></a><em>Interlude:</em> Constructing Optimal Stacked Regressions with <code>sl3</code>
</h3>
<p>To easily incorporate ensemble machine learning into the estimation procedure, we rely on the facilities provided in the <a href="https://tlverse.org/sl3"><code>sl3</code> R package</a>. For a complete guide on using the <code>sl3</code> R package, consider consulting <a href="https://tlverse.org/sl3" class="uri">https://tlverse.org/sl3</a>, or <a href="https://tlverse.org" class="uri">https://tlverse.org</a> for the <a href="https://github.com/tlverse"><code>tlverse</code> ecosystem</a>, of which <code>sl3</code> is a core component.</p>
<p>Using the framework provided by the <a href="https://tlverse.org/sl3"><code>sl3</code> package</a>, the nuisance parameters of the TML estimator may be fit with ensemble learning, using the cross-validation framework of the Super Learner algorithm of <span class="citation">van der Laan, Polley, and Hubbard (2007)</span>. To estimate the treatment mechanism (denoted g, we must make use of learning algorithms specifically suited to conditional density estimation; a list of such learners may be extracted from <code>sl3</code> using <code><a href="https://tlverse.org/sl3/reference/list_learners.html">sl3_list_learners()</a></code>:</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://tlverse.org/sl3/reference/list_learners.html">sl3_list_learners</a></span><span class="op">(</span><span class="st">"density"</span><span class="op">)</span></code></pre></div>
<pre><code>## [1] "Lrnr_density_discretize"     "Lrnr_density_hse"           
## [3] "Lrnr_density_semiparametric" "Lrnr_haldensify"            
## [5] "Lrnr_solnp_density"</code></pre>
<p>To proceed, we’ll select two of the above learners, <code>Lrnr_haldensify</code> for using the highly adaptive lasso for conditional density estimation, based on an algorithm given by <span class="citation">Dı́az and van der Laan (2011)</span>, and <code>Lrnr_density_semiparametric</code>, an approach for semiparametric conditional density estimation:</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># learners used for conditional density regression (i.e., propensity score)</span>
<span class="va">haldensify_lrnr</span> <span class="op">&lt;-</span> <span class="va"><a href="https://tlverse.org/sl3/reference/Lrnr_haldensify.html">Lrnr_haldensify</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span>
  n_bins <span class="op">=</span> <span class="fl">3</span>, grid_type <span class="op">=</span> <span class="st">"equal_mass"</span>,
  lambda_seq <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">exp</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="op">-</span><span class="fl">1</span>, <span class="op">-</span><span class="fl">9</span>, length <span class="op">=</span> <span class="fl">100</span><span class="op">)</span><span class="op">)</span>
<span class="op">)</span>
<span class="va">hse_lrnr</span> <span class="op">&lt;-</span> <span class="va"><a href="https://tlverse.org/sl3/reference/Lrnr_density_semiparametric.html">Lrnr_density_semiparametric</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span>mean_learner <span class="op">=</span> <span class="va"><a href="https://tlverse.org/sl3/reference/Lrnr_glm.html">Lrnr_glm</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span><span class="op">)</span><span class="op">)</span>
<span class="va">mvd_lrnr</span> <span class="op">&lt;-</span> <span class="va"><a href="https://tlverse.org/sl3/reference/Lrnr_density_semiparametric.html">Lrnr_density_semiparametric</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span>mean_learner <span class="op">=</span> <span class="va"><a href="https://tlverse.org/sl3/reference/Lrnr_glm.html">Lrnr_glm</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span><span class="op">)</span>,
                                            var_learner <span class="op">=</span> <span class="va"><a href="https://tlverse.org/sl3/reference/Lrnr_mean.html">Lrnr_mean</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span><span class="op">)</span><span class="op">)</span>
<span class="va">sl_lrn_dens</span> <span class="op">&lt;-</span> <span class="va"><a href="https://tlverse.org/sl3/reference/Lrnr_sl.html">Lrnr_sl</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span>
  learners <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span><span class="va">haldensify_lrnr</span>, <span class="va">hse_lrnr</span>, <span class="va">mvd_lrnr</span><span class="op">)</span>,
  metalearner <span class="op">=</span> <span class="va"><a href="https://tlverse.org/sl3/reference/Lrnr_solnp_density.html">Lrnr_solnp_density</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span><span class="op">)</span>
<span class="op">)</span></code></pre></div>
<p>We also require an approach for estimating the outcome regression (denoted Q). For this, we build a Super Learner composed of an intercept model, a main terms GLM, and the <a href="https://xgboost.ai/">xgboost algorithm</a> for gradient boosting:</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># learners used for conditional expectation regression (e.g., outcome)</span>
<span class="va">mean_lrnr</span> <span class="op">&lt;-</span> <span class="va"><a href="https://tlverse.org/sl3/reference/Lrnr_mean.html">Lrnr_mean</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span><span class="op">)</span>
<span class="va">glm_lrnr</span> <span class="op">&lt;-</span> <span class="va"><a href="https://tlverse.org/sl3/reference/Lrnr_glm.html">Lrnr_glm</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span><span class="op">)</span>
<span class="va">xgb_lrnr</span> <span class="op">&lt;-</span> <span class="va"><a href="https://tlverse.org/sl3/reference/Lrnr_xgboost.html">Lrnr_xgboost</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span><span class="op">)</span>
<span class="va">sl_lrn</span> <span class="op">&lt;-</span> <span class="va"><a href="https://tlverse.org/sl3/reference/Lrnr_sl.html">Lrnr_sl</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span>
  learners <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span><span class="va">mean_lrnr</span>, <span class="va">glm_lrnr</span>, <span class="va">xgb_lrnr</span><span class="op">)</span>,
  metalearner <span class="op">=</span> <span class="va"><a href="https://tlverse.org/sl3/reference/Lrnr_nnls.html">Lrnr_nnls</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span><span class="op">)</span>
<span class="op">)</span></code></pre></div>
<p>We can make the above explicit with respect to standard notation by bundling the ensemble learners into a <code>list</code> object below.</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># specify outcome and treatment regressions and create learner list</span>
<span class="va">Q_learner</span> <span class="op">&lt;-</span> <span class="va">sl_lrn</span>
<span class="va">g_learner</span> <span class="op">&lt;-</span> <span class="va">sl_lrn_dens</span>
<span class="va">learner_list</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>Y <span class="op">=</span> <span class="va">Q_learner</span>, A <span class="op">=</span> <span class="va">g_learner</span><span class="op">)</span></code></pre></div>
<p>The <code>learner_list</code> object above specifies the role that each of the ensemble learners we’ve generated is to play in computing initial estimators to be used in building a TMLE for the parameter of interest here. In particular, it makes explicit the fact that our <code>Q_learner</code> is used in fitting the outcome regression while our <code>g_learner</code> is used in fitting our treatment mechanism regression.</p>
</div>
<div id="targeted-estimation-of-stochastic-interventions-effects" class="section level3">
<h3 class="hasAnchor">
<a href="#targeted-estimation-of-stochastic-interventions-effects" class="anchor"></a>Targeted Estimation of Stochastic Interventions Effects</h3>
<p>Note that, by default, the</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">tmle_fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="http://tlverse.org/tmle3/reference/tmle3.html">tmle3</a></span><span class="op">(</span><span class="va">tmle_spec</span>, <span class="va">data</span>, <span class="va">node_list</span>, <span class="va">learner_list</span><span class="op">)</span></code></pre></div>
<pre><code>## Error : Optimal lambda not selected by CV in fitted haldensify model
## Error : Optimal lambda not selected by CV in fitted haldensify model
## Error : Optimal lambda not selected by CV in fitted haldensify model
## Error : Optimal lambda not selected by CV in fitted haldensify model
## Error : Optimal lambda not selected by CV in fitted haldensify model
## Error : Optimal lambda not selected by CV in fitted haldensify model
## Error : Optimal lambda not selected by CV in fitted haldensify model
## Error : Optimal lambda not selected by CV in fitted haldensify model
## Error : Optimal lambda not selected by CV in fitted haldensify model
## Error : Optimal lambda not selected by CV in fitted haldensify model
## 
## Iter: 1 fn: 1419.0430     Pars:  0.75107 0.24893
## Iter: 2 fn: 1419.0430     Pars:  0.75107 0.24893
## solnp--&gt; Completed in 2 iterations
## Error : Optimal lambda not selected by CV in fitted haldensify model
## Error : Optimal lambda not selected by CV in fitted haldensify model
## Error : Optimal lambda not selected by CV in fitted haldensify model
## Error : Optimal lambda not selected by CV in fitted haldensify model
## Error : Optimal lambda not selected by CV in fitted haldensify model
## Error : Optimal lambda not selected by CV in fitted haldensify model
## Error : Optimal lambda not selected by CV in fitted haldensify model
## Error : Optimal lambda not selected by CV in fitted haldensify model
## Error : Optimal lambda not selected by CV in fitted haldensify model
## Error : Optimal lambda not selected by CV in fitted haldensify model
## Error : Optimal lambda not selected by CV in fitted haldensify model
## Error : Optimal lambda not selected by CV in fitted haldensify model
## Error : Optimal lambda not selected by CV in fitted haldensify model
## Error : Optimal lambda not selected by CV in fitted haldensify model
## Error : Optimal lambda not selected by CV in fitted haldensify model
## Error : Optimal lambda not selected by CV in fitted haldensify model
## Error : Optimal lambda not selected by CV in fitted haldensify model
## Error : Optimal lambda not selected by CV in fitted haldensify model
## Error : Optimal lambda not selected by CV in fitted haldensify model
## Error : Optimal lambda not selected by CV in fitted haldensify model
## Error : Optimal lambda not selected by CV in fitted haldensify model
## Error : Optimal lambda not selected by CV in fitted haldensify model
## Error : Optimal lambda not selected by CV in fitted haldensify model
## Error : Optimal lambda not selected by CV in fitted haldensify model
## Error : Optimal lambda not selected by CV in fitted haldensify model
## Error : Optimal lambda not selected by CV in fitted haldensify model
## Error : Optimal lambda not selected by CV in fitted haldensify model
## Error : Optimal lambda not selected by CV in fitted haldensify model
## Error : Optimal lambda not selected by CV in fitted haldensify model
## Error : Optimal lambda not selected by CV in fitted haldensify model
## Error : Optimal lambda not selected by CV in fitted haldensify model
## Error : Optimal lambda not selected by CV in fitted haldensify model
## Error : Optimal lambda not selected by CV in fitted haldensify model
## Error : Optimal lambda not selected by CV in fitted haldensify model
## Error : Optimal lambda not selected by CV in fitted haldensify model
## Error : Optimal lambda not selected by CV in fitted haldensify model
## Error : Optimal lambda not selected by CV in fitted haldensify model
## Error : Optimal lambda not selected by CV in fitted haldensify model
## Error : Optimal lambda not selected by CV in fitted haldensify model
## Error : Optimal lambda not selected by CV in fitted haldensify model
## Error : Optimal lambda not selected by CV in fitted haldensify model
## Error : Optimal lambda not selected by CV in fitted haldensify model
## Error : Optimal lambda not selected by CV in fitted haldensify model
## Error : Optimal lambda not selected by CV in fitted haldensify model</code></pre>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">tmle_fit</span></code></pre></div>
<pre><code>## A tmle3_Fit that took 1 step(s)
##    type         param init_est tmle_est         se    lower    upper
## 1:  TSM E[Y_{A=NULL}] 2.051413  2.05467 0.06087795 1.935351 2.173989
##    psi_transformed lower_transformed upper_transformed
## 1:         2.05467          1.935351          2.173989</code></pre>
<p>The <code>print</code> method of the resultant <code>tmle_fit</code> object conveniently displays the results from computing our TML estimator.</p>
</div>
<div id="statistical-inference-for-targeted-maximum-likelihood-estimates" class="section level3">
<h3 class="hasAnchor">
<a href="#statistical-inference-for-targeted-maximum-likelihood-estimates" class="anchor"></a>Statistical Inference for Targeted Maximum Likelihood Estimates</h3>
<p>Recall that the asymptotic distribution of TML estimators has been studied thoroughly: <span class="math display">\[\psi_n - \psi_0 = (P_n - P_0) \cdot D(\bar{Q}_n^*, g_n) + R(\hat{P}^*, P_0),\]</span> which, provided the following two conditions:</p>
<ol style="list-style-type: decimal">
<li>If <span class="math inline">\(D(\bar{Q}_n^{\star}, g_n)\)</span> converges to <span class="math inline">\(D(P_0)\)</span> in <span class="math inline">\(L_2(P_0)\)</span> norm, and</li>
<li>the size of the class of functions considered for estimation of <span class="math inline">\(\bar{Q}_n^{\star}\)</span> and <span class="math inline">\(g_n\)</span> is bounded (technically, <span class="math inline">\(\exists \mathcal{F}\)</span> s.t. <span class="math inline">\(D(\bar{Q}_n^{\star}, g_n) \in \mathcal{F}\)</span> <em><strong>whp</strong></em>, where <span class="math inline">\(\mathcal{F}\)</span> is a Donsker class), readily admits the conclusion that <span class="math inline">\(\psi_n - \psi_0 = (P_n - P_0) \cdot D(P_0) + R(\hat{P}^{\star}, P_0)\)</span>.</li>
</ol>
<p>Under the additional condition that the remainder term <span class="math inline">\(R(\hat{P}^*, P_0)\)</span> decays as <span class="math inline">\(o_P \left( \frac{1}{\sqrt{n}} \right),\)</span> we have that <span class="math display">\[\psi_n - \psi_0 = (P_n - P_0) \cdot D(P_0) + o_P \left( \frac{1}{\sqrt{n}}
 \right),\]</span> which, by a central limit theorem, establishes a Gaussian limiting distribution for the estimator:</p>
<p><span class="math display">\[\sqrt{n}(\psi_n - \psi) \to N(0, V(D(P_0))),\]</span> where <span class="math inline">\(V(D(P_0))\)</span> is the variance of the efficient influence curve (canonical gradient) when <span class="math inline">\(\psi\)</span> admits an asymptotically linear representation.</p>
<p>The above implies that <span class="math inline">\(\psi_n\)</span> is a <span class="math inline">\(\sqrt{n}\)</span>-consistent estimator of <span class="math inline">\(\psi\)</span>, that it is asymptotically normal (as given above), and that it is locally efficient. This allows us to build Wald-type confidence intervals in a straightforward manner:</p>
<p><span class="math display">\[\psi_n \pm z_{\alpha} \cdot \frac{\sigma_n}{\sqrt{n}},\]</span> where <span class="math inline">\(\sigma_n^2\)</span> is an estimator of <span class="math inline">\(V(D(P_0))\)</span>. The estimator <span class="math inline">\(\sigma_n^2\)</span> may be obtained using the bootstrap or computed directly via the following</p>
<p><span class="math display">\[\sigma_n^2 = \frac{1}{n} \sum_{i = 1}^{n} D^2(\bar{Q}_n^{\star}, g_n)(O_i)\]</span></p>
<p>Having now re-examined these facts, let’s simply examine the results of computing our TML estimator:</p>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">tmle_fit</span></code></pre></div>
<pre><code>## A tmle3_Fit that took 1 step(s)
##    type         param init_est tmle_est         se    lower    upper
## 1:  TSM E[Y_{A=NULL}] 2.051413  2.05467 0.06087795 1.935351 2.173989
##    psi_transformed lower_transformed upper_transformed
## 1:         2.05467          1.935351          2.173989</code></pre>
</div>
</div>
<div id="references" class="section level2 unnumbered">
<h2 class="hasAnchor">
<a href="#references" class="anchor"></a>References</h2>
<div id="refs" class="references">
<div id="ref-diaz2011super">
<p>Dı́az, Iván, and Mark J van der Laan. 2011. “Super Learner Based Conditional Density Estimation with Application to Marginal Structural Models.” <em>The International Journal of Biostatistics</em> 7 (1): 1–20.</p>
</div>
<div id="ref-diaz2012population">
<p>———. 2012. “Population Intervention Causal Effects Based on Stochastic Interventions.” <em>Biometrics</em> 68 (2): 541–49.</p>
</div>
<div id="ref-diaz2018stochastic">
<p>———. 2018. “Stochastic Treatment Regimes.” In <em>Targeted Learning in Data Science: Causal Inference for Complex Longitudinal Studies</em>, 167–80. Springer Science &amp; Business Media.</p>
</div>
<div id="ref-pearl2000causality">
<p>Pearl, Judea. 2000. <em>Causality</em>. Cambridge university press.</p>
</div>
<div id="ref-vdl2007super">
<p>van der Laan, Mark J, Eric C Polley, and Alan E Hubbard. 2007. “Super Learner.” <em>Statistical Applications in Genetics and Molecular Biology</em> 6 (1).</p>
</div>
<div id="ref-vdl2011targeted">
<p>van der Laan, Mark J, and Sherri Rose. 2011. <em>Targeted Learning: Causal Inference for Observational and Experimental Data</em>. Springer Science &amp; Business Media.</p>
</div>
<div id="ref-vdl2018targeted">
<p>———. 2018. <em>Targeted Learning in Data Science: Causal Inference for Complex Longitudinal Studies</em>. Springer Science &amp; Business Media.</p>
</div>
<div id="ref-zheng2011cross">
<p>Zheng, Wenjing, and Mark J van der Laan. 2011. “Cross-Validated Targeted Minimum-Loss-Based Estimation.” In <em>Targeted Learning</em>, 459–74. Springer.</p>
</div>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

        <nav id="toc" data-toggle="toc"><h2 data-toc-skip>Contents</h2>
    </nav>
</div>

</div>



      <footer><div class="copyright">
  <p>Developed by Nima Hejazi, Jeremy Coyle, Mark van der Laan.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.6.1.</p>
</div>

      </footer>
</div>

  


  </body>
</html>
